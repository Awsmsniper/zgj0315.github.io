#+TITLE: 机器学习
* 概述
本文档记录了一个小白学习MLLib的过程
* 从数据中抽取合适的特征
** 从MovieLens数据集提取特征
#+BEGIN_SRC
u.item     -- Information about the items (movies); this is a tab separated
              list of
              movie id | movie title | release date | video release date |
              IMDb URL | unknown | Action | Adventure | Animation |
              Children's | Comedy | Crime | Documentary | Drama | Fantasy |
              Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |
              Thriller | War | Western |
              The last 19 fields are the genres, a 1 indicates the movie
              is of that genre, a 0 indicates it is not; movies can be in
              several genres at once.
              The movie ids are the ones used in the u.data data set.

val movies = sc.textFile("/Users/zhaogj/work/datasets/movieslens/ml-100k/u.item")
val movies = sc.textFile("/user/zhaogj/input/ml-100k/u.item")
println(movies.first)

u.genre    -- A list of the genres.

val genres = sc.textFile("/Users/zhaogj/work/datasets/movieslens/ml-100k/u.genre")
val genres = sc.textFile("/user/zhaogj/input/ml-100k/u.genre")
genres.take(5).foreach(println)

val genreMap = genres.filter(!_.isEmpty).map(line => line.split("\\|")).map(array => (array(1), array(0))).collectAsMap
println(genreMap)

val titlesAndGenres = movies.map(_.split("\\|")).map { array =>
  val genres = array.toSeq.slice(5, array.size)
  val genresAssigned = genres.zipWithIndex.filter { case (g, idx) =>
    g == "1"
  }.map { case (g, idx) =>
    genreMap(idx.toString)
  }
  (array(0).toInt, (array(1), genresAssigned))
}
println(titlesAndGenres.first)

import org.apache.spark.mllib.recommendation.ALS
import org.apache.spark.mllib.recommendation.Rating

u.data     -- The full u data set, 100000 ratings by 943 users on 1682 items.
              Each user has rated at least 20 movies.  Users and items are
              numbered consecutively from 1.  The data is randomly
              ordered. This is a tab separated list of 
	         user id | item id | rating | timestamp. 
              The time stamps are unix seconds since 1/1/1970 UTC   

val rawData = sc.textFile("/Users/zhaogj/work/datasets/movieslens/ml-100k/u.data")
val rawData = sc.textFile("/user/zhaogj/input/ml-100k/u.data")
println(rawData.first)

val rawRatings = rawData.map(_.split("\t").take(3))
rawRatings.collect

val ratings = rawRatings.map{ case Array(user, movie, rating) =>
  Rating(user.toInt, movie.toInt, rating.toDouble)
}
ratings.collect

ratings.cache
val alsModel = ALS.train(ratings, 50, 10, 0.1)

import org.apache.spark.mllib.linalg.Vectors
val movieFactors = alsModel.productFeatures.map { case (id, factor) =>
  (id, Vectors.dense(factor))
}
val movieVectors = movieFactors.map(_._2)
val userFactors = alsModel.userFeatures.map { case (id, factor) =>
  (id, Vectors.dense(factor))
}
val userVectors = userFactors.map(_._2)

import org.apache.spark.mllib.linalg.distributed.RowMatrix
val movieMatrix = new RowMatrix(movieVectors)
val movieMatrixSummary = movieMatrix.computeColumnSummaryStatistics()

val userMatrix = new RowMatrix(userVectors)
val userMatrixSummary = userMatrix.computeColumnSummaryStatistics()

println("Movie factors mean: " + movieMatrixSummary.mean)
println("Movie factors variance: " + movieMatrixSummary.variance)
println("User factors mean: " + userMatrixSummary.mean)
println("User factors variance: " + userMatrixSummary.variance)

import org.apache.spark.mllib.clustering.KMeans
val numClusters = 5
val numIterations = 10
val numRuns = 3

val movieClusterModel = KMeans.train(movieVectors, numClusters, numIterations, numRuns)

val movieClusterModel = KMeans.train(movieVectors, numClusters, 100)

val userClusterModel = KMeans.train(userVectors, numClusters, numIterations, numRuns)

val movie1 = movieVectors.first
val movieCluster = movieClusterModel.predict(movie1)
println(movieCluster)

val predictions = movieClusterModel.predict(movieVectors)
println(predictions.take(10).mkString(","))

import breeze.linalg._
import breeze.numerics.pow
def computeDistance(v1: DenseVector[Double], v2: DenseVector[Double])
  = pow(v1 -v2, 2).sum

val titlesWithFactors = titlesAndGenres.join(movieFactors)
val moviesAssigned = titlesWithFactors.map { case (id, ((title, genres), vector)) =>
  val pred = movieClusterModel.predict(vector)
  val clusterCentre = movieClusterModel.clusterCenters(pred)
  val dist = computeDistance(DenseVector(clusterCentre.toArray), DenseVector(vector.toArray))
    (id, title, genres.mkString(" "), pred, dist)
}
val clusterAssignments = moviesAssigned.groupBy{ case (id, title, genres, cluster, dist) =>
  cluster
}.collectAsMap

for ( (k, v) <- clusterAssignments.toSeq.sortBy(_._1)) {
  println(s"Cluster $k:")
  val m = v.toSeq.sortBy(_._5)
  println(m.take(20).map { case (_, title, genres, _, d) =>
    (title, genres, d)
  }.mkString("\n"))
  println("====\n")
}

val movieCost = movieClusterModel.computeCost(movieVectors)
val userCost = userClusterModel.computeCost(userVectors)
println("WCSS for movies: " + movieCost)
println("WCSS for users: " + userCost)

val trainTestSplitMovies = movieVectors.randomSplit(Array(0.6, 0.4), 123)
val trainMovies = trainTestSplitMovies(0)
val testMovies = trainTestSplitMovies(1)
val costsMovies = Seq(2, 3, 4, 5, 10, 20).map { k => 
  (k, KMeans.train(trainMovies, numIterations, k, numRuns).computeCost(testMovies))
}
println("Movie clustering cross-validation:")
costsMovies.foreach { case (k, cost) =>
  println(f"WCSS for K=$k id $cost%2.2f")
}


#+END_SRC
